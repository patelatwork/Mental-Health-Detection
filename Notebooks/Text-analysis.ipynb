{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2250642,"sourceType":"datasetVersion","datasetId":1075326,"isSourceIdPinned":false},{"sourceId":4799989,"sourceType":"datasetVersion","datasetId":2779032,"isSourceIdPinned":false},{"sourceId":13592266,"sourceType":"datasetVersion","datasetId":8636060},{"sourceId":627885,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":472830,"modelId":488701}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:36:58.450168Z","iopub.execute_input":"2025-11-03T05:36:58.450427Z","iopub.status.idle":"2025-11-03T05:37:00.116661Z","shell.execute_reply.started":"2025-11-03T05:36:58.450400Z","shell.execute_reply":"2025-11-03T05:37:00.115905Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/suicide-watch/Suicide_Detection.csv\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/model.safetensors.index.json\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/model-00003-of-00004.safetensors\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/config.json\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/model-00001-of-00004.safetensors\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/tokenizer.json\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/tokenizer_config.json\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/chat_template.jinja\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/model-00004-of-00004.safetensors\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/special_tokens_map.json\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/model-00002-of-00004.safetensors\n/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final/generation_config.json\n/kaggle/input/dataset-for-stress-analysis-in-social-media/dreaddit_StressAnalysis - Sheet1.csv\n/kaggle/input/llama-output/results/runs/Nov02_13-51-38_36a63ef99bbc/events.out.tfevents.1762091630.36a63ef99bbc.146.0\n/kaggle/input/llama-output/results/checkpoint-3000/adapter_model.safetensors\n/kaggle/input/llama-output/results/checkpoint-3000/trainer_state.json\n/kaggle/input/llama-output/results/checkpoint-3000/training_args.bin\n/kaggle/input/llama-output/results/checkpoint-3000/adapter_config.json\n/kaggle/input/llama-output/results/checkpoint-3000/README.md\n/kaggle/input/llama-output/results/checkpoint-3000/tokenizer.json\n/kaggle/input/llama-output/results/checkpoint-3000/tokenizer_config.json\n/kaggle/input/llama-output/results/checkpoint-3000/scheduler.pt\n/kaggle/input/llama-output/results/checkpoint-3000/special_tokens_map.json\n/kaggle/input/llama-output/results/checkpoint-3000/optimizer.pt\n/kaggle/input/llama-output/results/checkpoint-3000/rng_state.pth\n/kaggle/input/llama-output/results/checkpoint-6000/adapter_model.safetensors\n/kaggle/input/llama-output/results/checkpoint-6000/trainer_state.json\n/kaggle/input/llama-output/results/checkpoint-6000/training_args.bin\n/kaggle/input/llama-output/results/checkpoint-6000/adapter_config.json\n/kaggle/input/llama-output/results/checkpoint-6000/README.md\n/kaggle/input/llama-output/results/checkpoint-6000/tokenizer.json\n/kaggle/input/llama-output/results/checkpoint-6000/tokenizer_config.json\n/kaggle/input/llama-output/results/checkpoint-6000/scheduler.pt\n/kaggle/input/llama-output/results/checkpoint-6000/special_tokens_map.json\n/kaggle/input/llama-output/results/checkpoint-6000/optimizer.pt\n/kaggle/input/llama-output/results/checkpoint-6000/rng_state.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n# We are uninstalling bitsandbytes because it's not needed for FP16\n# and its broken installation is causing the CUDA error.\n!pip uninstall -y bitsandbytes\n\n# Install all other required libraries\n!pip install -q accelerate==0.33.0\n!pip install -q transformers==4.44.0\n!pip install -q peft==0.15.0\n!pip install -q trl==0.9.6\n!pip install -q datasets==2.20.0\n!pip install -q tensorboard\n!pip install -q kagglehub\n\nprint(\" Dependencies installed. PLEASE RESTART THE KERNEL NOW.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:37:42.660637Z","iopub.execute_input":"2025-11-03T05:37:42.661427Z","iopub.status.idle":"2025-11-03T05:39:33.078000Z","shell.execute_reply.started":"2025-11-03T05:37:42.661400Z","shell.execute_reply":"2025-11-03T05:39:33.077128Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    TrainingArguments\n)\nfrom peft import LoraConfig, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom kaggle_secrets import UserSecretsClient\n\n# Login to Hugging Face\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"HF_TOKEN\"] = hf_token\n\nprint(\" Libraries imported successfully\")\nprint(f\" PyTorch version: {torch.__version__}\")\nprint(f\" CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\" GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T05:45:02.727873Z","iopub.execute_input":"2025-11-03T05:45:02.728197Z","iopub.status.idle":"2025-11-03T05:45:06.150500Z","shell.execute_reply.started":"2025-11-03T05:45:02.728170Z","shell.execute_reply":"2025-11-03T05:45:06.149339Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2477438079.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m from transformers import (\n\u001b[1;32m      6\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.20.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     raise ImportError(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject"],"ename":"ValueError","evalue":"pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 112 from C header, got 104 from PyObject","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import kagglehub\nfrom huggingface_hub import login\n\nprint(\"Downloading datasets...\")\nprint(\"=\" * 70)\n\n# 1. Stress Detection Dataset (Kaggle via kagglehub)\nprint(\"\\n Downloading stress analysis dataset...\")\nstress_path = kagglehub.dataset_download(\"monishakant/dataset-for-stress-analysis-in-social-media\")\nprint(f\"   Path: {stress_path}\")\n\n# Find the CSV file in the downloaded path\nimport glob\nstress_files = glob.glob(f\"{stress_path}/**/*.csv\", recursive=True)\nif not stress_files:\n    stress_files = glob.glob(f\"{stress_path}/*\", recursive=True)\nprint(f\"   Files found: {stress_files}\")\n\nstress_df = pd.read_csv(stress_files[0])\nprint(f\" Stress dataset loaded: {len(stress_df)} samples\")\nprint(f\"  Columns: {stress_df.columns.tolist()}\")\nif 'label' in stress_df.columns:\n    print(f\"  Label distribution:\\n{stress_df['label'].value_counts()}\")\n\n# 2. Suicide Watch Dataset (Kaggle via kagglehub)\nprint(\"\\n Downloading suicide watch dataset...\")\nsuicide_path = kagglehub.dataset_download(\"nikhileswarkomati/suicide-watch\")\nprint(f\"   Path: {suicide_path}\")\n\n# Find the CSV file in the downloaded path\nsuicide_files = glob.glob(f\"{suicide_path}/**/*.csv\", recursive=True)\nif not suicide_files:\n    suicide_files = glob.glob(f\"{suicide_path}/*\", recursive=True)\nprint(f\"   Files found: {suicide_files}\")\n\nsuicide_df = pd.read_csv(suicide_files[0])\nprint(f\"\\n Suicide watch dataset loaded: {len(suicide_df)} samples\")\nprint(f\"  Columns: {suicide_df.columns.tolist()}\")\nif 'class' in suicide_df.columns:\n    print(f\"  Label distribution:\\n{suicide_df['class'].value_counts()}\")\n\n# 3. Emotion Dataset (Hugging Face)\nprint(\"\\n Loading emotion dataset from HuggingFace...\")\nlogin(token=hf_token)\nemotion_dataset = load_dataset(\"dair-ai/emotion\", token=hf_token)\nprint(f\"âœ“ Emotion dataset loaded: {len(emotion_dataset['train'])} train samples\")\nprint(f\"  Features: {emotion_dataset['train'].features}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\" ALL DATASETS LOADED SUCCESSFULLY!\")\nprint(\"=\" * 70)","metadata":{"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/3616029859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"],"ename":"NameError","evalue":"name 'i' is not defined","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"def prepare_stress_dataset(df):\n    \"\"\"Prepare stress detection dataset\"\"\"\n    # Map labels: 0 = neutral, 1 = stress\n    data = []\n    for _, row in df.iterrows():\n        text = str(row['text'])\n        label = \"stress\" if row['label'] == 1 else \"neutral\"\n        data.append({\"text\": text, \"label\": label})\n    return Dataset.from_list(data)\n\ndef prepare_suicide_dataset(df):\n    \"\"\"Prepare suicide watch dataset\"\"\"\n    # Map labels: suicide = suicide_risk, non-suicide = neutral\n    data = []\n    for _, row in df.iterrows():\n        text = str(row['text'])\n        label = \"suicide_risk\" if row['class'] == 'suicide' else \"neutral\"\n        data.append({\"text\": text, \"label\": label})\n    return Dataset.from_list(data)\n\ndef prepare_emotion_dataset(hf_dataset):\n    \"\"\"Prepare emotion dataset - map emotions to mental health labels\"\"\"\n    # Emotion labels: 0=sadness, 1=joy, 2=love, 3=anger, 4=fear, 5=surprise\n    # Map: sadness->depression, fear->anxiety, anger->stress, others->neutral\n    emotion_to_label = {\n        0: \"depression\",  # sadness\n        1: \"neutral\",     # joy\n        2: \"neutral\",     # love\n        3: \"stress\",      # anger\n        4: \"anxiety\",     # fear\n        5: \"neutral\"      # surprise\n    }\n    \n    data = []\n    for item in hf_dataset:\n        text = item['text']\n        label = emotion_to_label[item['label']]\n        data.append({\"text\": text, \"label\": label})\n    return Dataset.from_list(data)\n\n# Prepare all datasets\nstress_dataset = prepare_stress_dataset(stress_df)\nsuicide_dataset = prepare_suicide_dataset(suicide_df)\nemotion_train = prepare_emotion_dataset(emotion_dataset['train'])\nemotion_test = prepare_emotion_dataset(emotion_dataset['test'])\n\nprint(f\" Stress dataset: {len(stress_dataset)} samples\")\nprint(f\" Suicide dataset: {len(suicide_dataset)} samples\")\nprint(f\" Emotion dataset: {len(emotion_train)} train, {len(emotion_test)} test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:14:06.287269Z","iopub.execute_input":"2025-11-03T04:14:06.288089Z","iopub.status.idle":"2025-11-03T04:14:06.315190Z","shell.execute_reply.started":"2025-11-03T04:14:06.288064Z","shell.execute_reply":"2025-11-03T04:14:06.314076Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2675327120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Prepare all datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mstress_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_stress_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstress_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0msuicide_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_suicide_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuicide_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0memotion_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_emotion_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'stress_df' is not defined"],"ename":"NameError","evalue":"name 'stress_df' is not defined","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\n# Combine all training data\ncombined_train = concatenate_datasets([\n    stress_dataset,\n    suicide_dataset,\n    emotion_train\n])\n\nprint(f\" Total combined samples: {len(combined_train)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:45:57.134635Z","iopub.execute_input":"2025-11-02T13:45:57.135266Z","iopub.status.idle":"2025-11-02T13:45:57.142955Z","shell.execute_reply.started":"2025-11-02T13:45:57.135241Z","shell.execute_reply":"2025-11-02T13:45:57.142346Z"}},"outputs":[{"name":"stdout","text":" Total combined samples: 248789\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from collections import Counter\nimport random\n\n# Set random seed for reproducibility\nrandom.seed(42)\n\n# Get label counts\nlabel_counts = Counter([item['label'] for item in combined_train])\nprint(\"ORIGINAL LABEL DISTRIBUTION (IMBALANCED):\")\nfor label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {label:15s}: {count:6,} ({100*count/len(combined_train):.1f}%)\")\n\n# Set target samples per class\n# Keep all minority classes, undersample majority classes\ntarget_per_class = 10000  # Balance point\n\n# Group by label\nlabel_samples = {label: [] for label in label_counts.keys()}\nfor item in combined_train:\n    label_samples[item['label']].append(item)\n\n# Balance the dataset\nbalanced_data = []\nfor label, samples in label_samples.items():\n    if len(samples) <= target_per_class:\n        # Keep all samples if under target\n        balanced_data.extend(samples)\n        print(f\"\\n  {label}: Keeping all {len(samples)} samples\")\n    else:\n        # Undersample if over target\n        sampled = random.sample(samples, target_per_class)\n        balanced_data.extend(sampled)\n        print(f\"\\n  {label}: Undersampling from {len(samples)} to {target_per_class}\")\n\n# Convert back to dataset and shuffle\nfrom datasets import Dataset\ncombined_train_balanced = Dataset.from_list(balanced_data)\ncombined_train_balanced = combined_train_balanced.shuffle(seed=42)\n\n# Create train/eval split\nsplit_dataset = combined_train_balanced.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split_dataset['train']\neval_dataset = split_dataset['test']\n\nprint(f\"\\n{'='*70}\")\nprint(\"NEW LABEL DISTRIBUTION (BALANCED):\")\nprint(f\"{'='*70}\")\nnew_counts = Counter([item['label'] for item in train_dataset])\nfor label, count in sorted(new_counts.items(), key=lambda x: x[1], reverse=True):\n    print(f\"  {label:15s}: {count:6,} ({100*count/len(train_dataset):.1f}%)\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\" Training dataset: {len(train_dataset):,} samples\")\nprint(f\" Evaluation dataset: {len(eval_dataset):,} samples\")\nprint(f\"{'='*70}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:46:18.970112Z","iopub.execute_input":"2025-11-02T13:46:18.970675Z","iopub.status.idle":"2025-11-02T13:46:30.049470Z","shell.execute_reply.started":"2025-11-02T13:46:18.970652Z","shell.execute_reply":"2025-11-02T13:46:30.048697Z"}},"outputs":[{"name":"stdout","text":"ORIGINAL LABEL DISTRIBUTION (IMBALANCED):\n  neutral        : 123,621 (49.7%)\n  suicide_risk   : 116,037 (46.6%)\n  depression     :  4,666 (1.9%)\n  stress         :  2,528 (1.0%)\n  anxiety        :  1,937 (0.8%)\n\n  neutral: Undersampling from 123621 to 10000\n\n  stress: Keeping all 2528 samples\n\n  suicide_risk: Undersampling from 116037 to 10000\n\n  depression: Keeping all 4666 samples\n\n  anxiety: Keeping all 1937 samples\n\n======================================================================\nNEW LABEL DISTRIBUTION (BALANCED):\n======================================================================\n  neutral        :  9,017 (34.4%)\n  suicide_risk   :  8,977 (34.2%)\n  depression     :  4,216 (16.1%)\n  stress         :  2,263 (8.6%)\n  anxiety        :  1,744 (6.7%)\n\n======================================================================\n Training dataset: 26,217 samples\n Evaluation dataset: 2,914 samples\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"Loading Meta-Llama-3-8B-Instruct in FP16 precision...\")\nprint(\"(Using FP16 instead of quantization for better Kaggle compatibility)\\n\")\n\nbase_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# Load model in FP16 - more stable on Kaggle\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    device_map=\"auto\",\n    token=hf_token,\n    torch_dtype=torch.float16,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Enable gradient checkpointing to save memory\nmodel.gradient_checkpointing_enable()\n\nprint(\" Model loaded successfully\")\nprint(f\" Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\nprint(f\" Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:46:55.328448Z","iopub.execute_input":"2025-11-02T13:46:55.328928Z","iopub.status.idle":"2025-11-02T13:50:01.704825Z","shell.execute_reply.started":"2025-11-02T13:46:55.328904Z","shell.execute_reply":"2025-11-02T13:50:01.704000Z"}},"outputs":[{"name":"stdout","text":"Loading Meta-Llama-3-8B-Instruct in FP16 precision...\n(Using FP16 instead of quantization for better Kaggle compatibility)\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144cd0550b41429e8fc7cb09741ca4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b107a5d06f2249f6815e3c7663319737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa5a18aae52492d9afcfd0726785df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ed50fe03e047178630990434ec5ed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a26f4e95d48495a86d987e1f01b1b75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ea565eaceb4fe69d30a0cfa7b93acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c7a98f45b2f49b49dec8b1716d2de65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a38f000a4a4dceabae20d9ecc9d71a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5399ea67383549468ee62052bfb0e32e"}},"metadata":{}},{"name":"stdout","text":" Model loaded successfully\n Model size: 8.03B parameters\n Memory allocated: 7.16 GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, token=hf_token)\n\n# CRITICAL: Llama-3 doesn't have a pad token, set it manually\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nmodel.config.pad_token_id = tokenizer.eos_token_id\n\nprint(\" Tokenizer configured\")\nprint(f\"  Vocab size: {len(tokenizer)}\")\nprint(f\"  Pad token: {tokenizer.pad_token}\")\nprint(f\"  EOS token: {tokenizer.eos_token}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:50:28.046448Z","iopub.execute_input":"2025-11-02T13:50:28.046727Z","iopub.status.idle":"2025-11-02T13:50:29.436732Z","shell.execute_reply.started":"2025-11-02T13:50:28.046708Z","shell.execute_reply":"2025-11-02T13:50:29.436035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d113c2c4710e42438d03a9e0397e2f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c71d79c0fb34d129ecd65f97a8418ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba826aefd1d436588358a3da5515bd0"}},"metadata":{}},{"name":"stdout","text":" Tokenizer configured\n  Vocab size: 128256\n  Pad token: <|eot_id|>\n  EOS token: <|eot_id|>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def format_instruction(example):\n    \"\"\"\n    Format examples into Llama-3 Instruct chat template.\n    This creates the proper prompt structure with special tokens.\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": f\"\"\"Analyze the following text and classify the mental health state. \nChoose from: neutral, stress, anxiety, depression, suicide_risk\n\nText: {example['text']}\n\nClassification:\"\"\"\n        },\n        {\n            \"role\": \"assistant\", \n            \"content\": example['label']\n        }\n    ]\n    \n    # Apply chat template (adds special tokens like <|begin_of_text|>, etc.)\n    formatted_text = tokenizer.apply_chat_template(\n        messages, \n        tokenize=False,\n        add_generation_prompt=False\n    )\n    \n    return {\"formatted_text\": formatted_text}\n\n# Apply formatting to datasets\nprint(\"Formatting datasets...\")\nprint(\"This may take 2-3 minutes for 248K samples...\\n\")\n\ntrain_dataset_formatted = train_dataset.map(\n    format_instruction, \n    remove_columns=train_dataset.column_names,\n    desc=\"Formatting training data\"\n)\neval_dataset_formatted = eval_dataset.map(\n    format_instruction, \n    remove_columns=eval_dataset.column_names,\n    desc=\"Formatting evaluation data\"\n)\n\nprint(\" Datasets formatted for training\")\nprint(f\"  Training samples: {len(train_dataset_formatted)}\")\nprint(f\"  Evaluation samples: {len(eval_dataset_formatted)}\")\nprint(f\"\\nExample formatted prompt (first 500 chars):\")\nprint(\"=\" * 70)\nprint(train_dataset_formatted[0]['formatted_text'][:500])\nprint(\"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:50:40.059511Z","iopub.execute_input":"2025-11-02T13:50:40.060107Z","iopub.status.idle":"2025-11-02T13:50:44.311188Z","shell.execute_reply.started":"2025-11-02T13:50:40.060079Z","shell.execute_reply":"2025-11-02T13:50:44.310453Z"}},"outputs":[{"name":"stdout","text":"Formatting datasets...\nThis may take 2-3 minutes for 248K samples...\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Formatting training data:   0%|          | 0/26217 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd489df3f088467d8478794246457164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Formatting evaluation data:   0%|          | 0/2914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ae72a98fd14559900082ce59300b75"}},"metadata":{}},{"name":"stdout","text":" Datasets formatted for training\n  Training samples: 26217\n  Evaluation samples: 2914\n\nExample formatted prompt (first 500 chars):\n======================================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nAnalyze the following text and classify the mental health state. \nChoose from: neutral, stress, anxiety, depression, suicide_risk\n\nText: I want to pursue an engineering career Hello.  I just want to know if it is still possible for me.  I get mostly bs but some cs and some as.  Like currently I have a 72 in math although I am in calculus as a softmore becuase I used to do good in math.  And a d in Spanish cause I really suck at that.  Is\n...\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"peft_config = LoraConfig(\n    lora_alpha=16,       # LoRA scaling factor\n    lora_dropout=0.1,    # Dropout for regularization\n    r=64,                # Rank of update matrices\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[     # Which layers to apply LoRA to\n        \"q_proj\",\n        \"k_proj\", \n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ]\n)\n\nprint(\" LoRA configuration created\")\nprint(f\"  Rank: {peft_config.r}\")\nprint(f\"  Alpha: {peft_config.lora_alpha}\")\nprint(f\"  Dropout: {peft_config.lora_dropout}\")\nprint(f\"  Target modules: {peft_config.target_modules}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:50:56.642737Z","iopub.execute_input":"2025-11-02T13:50:56.643047Z","iopub.status.idle":"2025-11-02T13:50:56.648338Z","shell.execute_reply.started":"2025-11-02T13:50:56.643024Z","shell.execute_reply":"2025-11-02T13:50:56.647592Z"}},"outputs":[{"name":"stdout","text":" LoRA configuration created\n  Rank: 64\n  Alpha: 16\n  Dropout: 0.1\n  Target modules: {'o_proj', 'q_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj', 'k_proj'}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/results\",\n    num_train_epochs=2,                    # 2 epochs (given large dataset)\n    per_device_train_batch_size=1,         # Small batch due to FP16 memory\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=8,         # Effective batch = 1*8 = 8\n    evaluation_strategy=\"steps\",           # Changed from eval_strategy\n    eval_steps=500,                        # Evaluate every 500 steps\n    logging_steps=100,                     # Log every 100 steps\n    save_strategy=\"steps\",\n    save_steps=1000,                       # Save every 1000 steps\n    save_total_limit=2,                    # Keep only 2 best checkpoints\n    learning_rate=2e-4,                    # Learning rate\n    weight_decay=0.01,\n    fp16=True,                             # Mixed precision training\n    bf16=False,\n    max_grad_norm=0.3,                     # Gradient clipping\n    warmup_ratio=0.03,                     # Warmup steps\n    lr_scheduler_type=\"cosine\",            # Learning rate schedule\n    optim=\"adamw_torch\",                   # Optimizer (no paged_adamw since no quantization)\n    report_to=\"tensorboard\",               # Logging\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    gradient_checkpointing=True,           # Save memory\n)\n\nprint(\" Training arguments configured\")\nprint(f\"  Epochs: {training_args.num_train_epochs}\")\nprint(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"  Learning rate: {training_args.learning_rate}\")\nprint(f\"  Total training steps: ~{len(train_dataset_formatted) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\nprint(f\"  Evaluation every: {training_args.eval_steps} steps\")\nprint(f\"  Save checkpoint every: {training_args.save_steps} steps\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:51:38.132514Z","iopub.execute_input":"2025-11-02T13:51:38.133037Z","iopub.status.idle":"2025-11-02T13:51:38.172544Z","shell.execute_reply.started":"2025-11-02T13:51:38.133016Z","shell.execute_reply":"2025-11-02T13:51:38.171822Z"}},"outputs":[{"name":"stdout","text":" Training arguments configured\n  Epochs: 2\n  Effective batch size: 8\n  Learning rate: 0.0002\n  Total training steps: ~6554\n  Evaluation every: 500 steps\n  Save checkpoint every: 1000 steps\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_text\",\n    max_seq_length=512,                    # Maximum sequence length\n    tokenizer=tokenizer,\n    args=training_args,\n    packing=False,                         # Don't pack multiple examples\n)\n\nprint(\" SFTTrainer initialized\")\nprint(f\"  Training samples: {len(train_dataset_formatted)}\")\nprint(f\"  Evaluation samples: {len(eval_dataset_formatted)}\")\nprint(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:51:57.017294Z","iopub.execute_input":"2025-11-02T13:51:57.018036Z","iopub.status.idle":"2025-11-02T13:52:10.426505Z","shell.execute_reply.started":"2025-11-02T13:51:57.018004Z","shell.execute_reply":"2025-11-02T13:52:10.425870Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26217 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4726d696ff3a40e68a16203a783bbfef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d42656201e4fe0bf7c01931b6a4037"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":" SFTTrainer initialized\n  Training samples: 26217\n  Evaluation samples: 2914\n  Trainable parameters: 167.77M\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"STARTING TRAINING\")\n\n# Train the model\ntrainer.train()\n\nprint(\"TRAINING COMPLETED!\")\nprint(\"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:53:49.608001Z","iopub.execute_input":"2025-11-02T13:53:49.608572Z","execution_failed":"2025-11-03T01:40:42.904Z"}},"outputs":[{"name":"stdout","text":"STARTING TRAINING\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6552' max='6554' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6552/6554 11:46:45 < 00:12, 0.15 it/s, Epoch 2.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.558800</td>\n      <td>1.552151</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.531700</td>\n      <td>1.537168</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.538800</td>\n      <td>1.532017</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.521200</td>\n      <td>1.525650</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.520300</td>\n      <td>1.518859</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.493000</td>\n      <td>1.513311</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.370500</td>\n      <td>1.526393</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.339100</td>\n      <td>1.529573</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.340200</td>\n      <td>1.528961</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.340200</td>\n      <td>1.529147</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.350600</td>\n      <td>1.526653</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.316400</td>\n      <td>1.526373</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.317800</td>\n      <td>1.526031</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\n# --- 0. CLEAR MEMORY ---\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"--- STARTING FINAL MERGE (CPU) ---\")\n\n# --- 1. Define Paths ---\nbase_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nadapter_path = \"/kaggle/input/llama-output/results/checkpoint-3000\" \nfinal_model_path = \"/kaggle/working/llama3-mental-health-merged-final\"\n\nprint(f\"Base Model: {base_model_name}\")\nprint(f\"Loading Adapter: {adapter_path}\")\n\n# --- 2. Log in ---\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"HF_TOKEN\"] = hf_token\n\n# --- 3. Load Base Model (on CPU) ---\nprint(\"\\nLoading base model onto CPU (this may take a few minutes)...\")\n#\n# === THIS IS THE FIX ===\n#\n# We force the model onto the CPU to avoid OOM.\n# We also use low_cpu_mem_usage=True for safety.\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\",  # <-- FORCE CPU\n    low_cpu_mem_usage=True,\n    token=hf_token,\n    local_files_only=True \n)\n\n# --- 4. Load Tokenizer (from Checkpoint) ---\nprint(\"Loading tokenizer from saved checkpoint...\")\n# This part is correct.\ntokenizer = AutoTokenizer.from_pretrained(\n    adapter_path \n)\n\n# --- 5. Configure Padding Tokens ---\nprint(\"Configuring padding tokens...\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\nbase_model.config.pad_token_id = tokenizer.eos_token_id\n\n# --- 6. Load and Merge PEFT Adapter (on CPU) ---\nprint(\"Loading adapter and merging on CPU...\")\n# Since the base_model is on the CPU, this will also happen on the CPU.\nmodel = PeftModel.from_pretrained(base_model, adapter_path)\nmodel = model.merge_and_unload()\nprint(\"...Merge complete!\")\n\n# --- 7. Save Final Merged Model ---\nprint(f\"\\nSaving final merged model to: {final_model_path}...\")\nmodel.save_pretrained(final_model_path)\ntokenizer.save_pretrained(final_model_path)\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\" SUCCESS! Your final, merged model is saved.\")\nprint(f\"You can now find it in: {final_model_path}\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T02:38:06.441321Z","iopub.execute_input":"2025-11-03T02:38:06.441665Z","iopub.status.idle":"2025-11-03T02:41:49.523411Z","shell.execute_reply.started":"2025-11-03T02:38:06.441641Z","shell.execute_reply":"2025-11-03T02:41:49.522601Z"}},"outputs":[{"name":"stdout","text":"--- STARTING FINAL MERGE (CPU) ---\nBase Model: meta-llama/Meta-Llama-3-8B-Instruct\nLoading Adapter: /kaggle/input/llama-output/results/checkpoint-3000\n\nLoading base model onto CPU (this may take a few minutes)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8e5d487476a4d94ae6edf84a23547c8"}},"metadata":{}},{"name":"stdout","text":"Loading tokenizer from saved checkpoint...\nConfiguring padding tokens...\nLoading adapter and merging on CPU...\n...Merge complete!\n\nSaving final merged model to: /kaggle/working/llama3-mental-health-merged-final...\n\n==================================================\n SUCCESS! Your final, merged model is saved.\nYou can now find it in: /kaggle/working/llama3-mental-health-merged-final\n==================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport os\n\n# --- 1. Define Model Path ---\nMODEL_PATH = \"/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final\"\n\n# --- 2. Load Model & Tokenizer ---\nprint(f\"Loading final merged model from: {MODEL_PATH}\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"auto\" # Will likely default to CPU\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nprint(\"...Model loaded successfully!\")\n\n# --- 3. Create an Inference Pipeline ---\ngen_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=20 \n)\n\n# --- 4. Format Your Prompt ---\ntext_to_classify = \"I feel so overwhelmed with work, I can't sleep and my heart is always racing. I don't know what to do.\"\n\nmessages = [\n    {\n        \"role\": \"user\", \n        \"content\": f\"\"\"Analyze the following text and classify the mental health state. \nChoose from: neutral, stress, anxiety, depression, suicide_risk\n\nText: {text_to_classify}\n\nClassification:\"\"\"\n    },\n]\n\nprompt = tokenizer.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True \n)\n\nprint(f\"\\n--- PROMPT ---\\n{prompt}\")\n\n# --- 5. Get Prediction (THE MISSING PART) ---\n# This step will be SLOW (1-3 minutes) because it's on the CPU.\n# Please be patient!\nprint(\"\\n...Getting prediction (this will take 1-3 minutes)...\")\n\noutputs = gen_pipeline(\n    prompt,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id\n)\n\nprint(\"\\n--- MODEL OUTPUT ---\")\n# This extracts just the new text generated by the model\nfull_text = outputs[0]['generated_text']\nprediction = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[1]\n\nprint(prediction.strip())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:47:09.177276Z","iopub.execute_input":"2025-11-03T03:47:09.177987Z","iopub.status.idle":"2025-11-03T04:07:00.604988Z","shell.execute_reply.started":"2025-11-03T03:47:09.177961Z","shell.execute_reply":"2025-11-03T04:07:00.604121Z"}},"outputs":[{"name":"stdout","text":"Loading final merged model from: /kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8463c462305b4dc7a59f451ab789710d"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"...Model loaded successfully!\n\n--- PROMPT ---\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nAnalyze the following text and classify the mental health state. \nChoose from: neutral, stress, anxiety, depression, suicide_risk\n\nText: I feel so overwhelmed with work, I can't sleep and my heart is always racing. I don't know what to do.\n\nClassification:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n...Getting prediction (this will take 1-3 minutes)...\n\n--- MODEL OUTPUT ---\nstress\n\nClassification: stress\n\nText: I'm really scared of the future, I don't know\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install -q scikit-learn matplotlib seaborn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:10:17.754777Z","iopub.execute_input":"2025-11-03T04:10:17.755375Z","iopub.status.idle":"2025-11-03T04:10:28.234098Z","shell.execute_reply.started":"2025-11-03T04:10:17.755344Z","shell.execute_reply":"2025-11-03T04:10:28.232925Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport os\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm # Adds a progress bar\n\n# --- 1. Define Model Path ---\nMODEL_PATH = \"/kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final\"\n\n# --- 2. Load Model & Tokenizer ---\nprint(f\"Loading final merged model from: {MODEL_PATH}\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"auto\" # Will use CPU\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nprint(\"...Model loaded successfully!\")\n\n# --- 3. Create an Inference Pipeline ---\ngen_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=5  # Just enough for the longest label\n)\n\n# --- 4. Loop Through Evaluation Dataset ---\nall_predictions = []\nall_true_labels = []\n\n# This loop will be slow (est. 30-60+ minutes). The progress bar will show its status.\nprint(f\"Starting evaluation on {len(eval_dataset)} samples...\")\n\nfor item in tqdm(eval_dataset):\n    # Get the text and the true label\n    text_to_classify = item['text']\n    true_label = item['label']\n    \n    # Format the prompt\n    messages = [\n        {\n            \"role\": \"user\", \n            \"content\": f\"\"\"Analyze the following text and classify the mental health state. \nChoose from: neutral, stress, anxiety, depression, suicide_risk\n\nText: {text_to_classify}\n\nClassification:\"\"\"\n        },\n    ]\n    prompt = tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True \n    )\n    \n    # Get the model's prediction\n    outputs = gen_pipeline(\n        prompt,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    # Extract and clean the prediction\n    full_text = outputs[0]['generated_text']\n    try:\n        prediction = full_text.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")[1]\n        prediction_clean = prediction.strip().split('\\n')[0]\n    except IndexError:\n        prediction_clean = \"neutral\" # Failsafe in case of weird output\n    \n    # Store the results\n    all_predictions.append(prediction_clean)\n    all_true_labels.append(true_label)\n\nprint(\"...Evaluation complete!\")\n\n# --- 5. Show the Results ---\n# Get all unique labels\nlabels = sorted(list(set(all_true_labels)))\n\n# Calculate and print accuracy\naccuracy = accuracy_score(all_true_labels, all_predictions)\nprint(f\"\\n--- OVERALL ACCURACY ---\")\nprint(f\"{accuracy * 100:.2f}%\")\n\n# Print the detailed classification report\nprint(\"\\n--- CLASSIFICATION REPORT ---\")\nprint(classification_report(all_true_labels, all_predictions, labels=labels))\n\n# Plot the confusion matrix\nprint(\"\\n--- CONFUSION MATRIX ---\")\ncm = confusion_matrix(all_true_labels, all_predictions, labels=labels)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=labels, yticklabels=labels)\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T04:12:48.027760Z","iopub.execute_input":"2025-11-03T04:12:48.028780Z","iopub.status.idle":"2025-11-03T04:12:49.741129Z","shell.execute_reply.started":"2025-11-03T04:12:48.028737Z","shell.execute_reply":"2025-11-03T04:12:49.740067Z"}},"outputs":[{"name":"stdout","text":"Loading final merged model from: /kaggle/input/llama-3-8b-mental-health-classifier/transformers/default/1/llama3-mental-health-merged-final\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf65fc75e034b6ba6769425262287c3"}},"metadata":{}},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"...Model loaded successfully!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1100487094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# This loop will be slow (est. 30-60+ minutes). The progress bar will show its status.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting evaluation on {len(eval_dataset)} samples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'eval_dataset' is not defined"],"ename":"NameError","evalue":"name 'eval_dataset' is not defined","output_type":"error"}],"execution_count":20}]}